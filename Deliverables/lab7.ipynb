{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords as sw\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using a pre-assembled dataset from `saved_sets`, skip to \"Train and Test\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderFile = open(\"order.pkl\",\"rb\")\n",
    "randomOrder = pickle.load(orderFile)\n",
    "orderFile.close()\n",
    "\n",
    "positiveTrainFile = open(\"aclImdb/train_pos.txt\",\"r\")\n",
    "negativeTrainFile = open(\"aclImdb/train_neg.txt\",\"r\")\n",
    "positiveTestFile = open(\"aclImdb/test_pos.txt\",\"r\")\n",
    "negativeTestFile = open(\"aclImdb/test_neg.txt\",\"r\")\n",
    "\n",
    "positiveTrainText = re.sub(\"(  +)|(<.*>)\", \" \", re.sub(\":|,|;|/|<br >|\\\"|\\.|\\(|\\)|\\*\", \"\", positiveTrainFile.read().lower()))\n",
    "negativeTrainText = re.sub(\"(  +)|(<.*>)\", \" \", re.sub(\":|,|;|/|<br >|\\\"|\\.|\\(|\\)|\\*\", \"\", negativeTrainFile.read().lower()))\n",
    "positiveTestText = re.sub(\"(  +)|(<.*>)\", \" \", re.sub(\":|,|;|/|<br >|\\\"|\\.|\\(|\\)|\\*\", \"\", positiveTestFile.read().lower()))\n",
    "negativeTestText = re.sub(\"(  +)|(<.*>)\", \" \", re.sub(\":|,|;|/|<br >|\\\"|\\.|\\(|\\)|\\*\", \"\", negativeTestFile.read().lower()))\n",
    "\n",
    "positiveTrainFile.close()\n",
    "negativeTrainFile.close()\n",
    "positiveTestFile.close()\n",
    "negativeTestFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineer and Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLines = [\"1 | \" + line for line in positiveTrainText.split(\"\\n\")[:-1]] + [\"-1 | \" + line for line in negativeTrainText.split(\"\\n\")[:-1]]\n",
    "testLines = [\"1 | \" + line for line in positiveTestText.split(\"\\n\")[:-1]] + [\"-1 | \" + line for line in negativeTestText.split(\"\\n\")[:-1]]\n",
    "stopwords = sw.words('english')\n",
    "nGramN = 1\n",
    "binaryFeatures = True\n",
    "removeStopwords = False\n",
    "addLengthFeature = True\n",
    "addICount = False\n",
    "addExclaimCount = False\n",
    "addFilmCount = False\n",
    "\n",
    "for lineSet in [trainLines, testLines]:\n",
    "    for i,line in enumerate(lineSet):\n",
    "        features = line.split(\" | \")[1].split(\" \")\n",
    "        nGrams = []\n",
    "        for j in range(1,nGramN + 1):\n",
    "            nGrams = nGrams + list(ngrams(features, j))\n",
    "        separator = \"_\"\n",
    "        nGrams = [separator.join(gram) for gram in nGrams]\n",
    "        if removeStopwords:\n",
    "            nGrams = [gram for gram in nGrams if any([word not in stopwords for word in gram.split(separator)])]\n",
    "        features = nGrams\n",
    "        if binaryFeatures:\n",
    "            if addICount:\n",
    "                nI = features.count(\"i\")\n",
    "                features.append(\"numberOfI:\" + str(nI))\n",
    "            if addExclaimCount:\n",
    "                nExclaim = 0\n",
    "                for feature in features:\n",
    "                    if \"!\" in feature:\n",
    "                        nExclaim += 1\n",
    "                features.append(\"numberOfExclaim:\" + str(nExclaim))\n",
    "            if addFilmCount:\n",
    "                nFilm = features.count(\"film\")\n",
    "                features.insert(0,\"numberOfFilm:\" + str(nFilm))\n",
    "        line = line.split(\" | \")[0] + \" | \" + \" \".join(features)\n",
    "        if not binaryFeatures:\n",
    "            counts = {}\n",
    "            for gram in nGrams:\n",
    "                if gram in counts:\n",
    "                    counts[gram] += 1\n",
    "                else:\n",
    "                    counts[gram] = 1\n",
    "            nonBinaryFeatures = [ f'{gram}:{counts[gram]}' for gram in counts]\n",
    "            features = nonBinaryFeatures\n",
    "        if addLengthFeature:\n",
    "            l = len(features)\n",
    "            features.append(\"length:\" + str(l))\n",
    "        line = line.split(\" | \")[0] + \" | \" + \" \".join(features)\n",
    "        if lineSet == trainLines:\n",
    "            trainLines[i] = line\n",
    "        elif lineSet == testLines:\n",
    "            testLines[i] = line\n",
    "\n",
    "tmp = []\n",
    "for i in randomOrder:\n",
    "    tmp.append(trainLines[i])\n",
    "trainLines = tmp\n",
    "tmp = []\n",
    "for i in randomOrder:\n",
    "    tmp.append(testLines[i])\n",
    "testLines = tmp\n",
    "\n",
    "trainFile = open(\"train.vw\",\"w\")\n",
    "testFile = open(\"test.vw\",\"w\")\n",
    "\n",
    "trainFile.write(\"\\n\".join(trainLines))\n",
    "testFile.write(\"\\n\".join(testLines))\n",
    "\n",
    "trainFile.close()\n",
    "testFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters set below yield the best accuracy I've been able to get. It uses bigrams and mostly binary features. Noticing that negative reviews tend to be shorter than positive reviews, I added a `length` feature that holds the length of the review. This didn't improve accuracy by much (only 0.32%), but I kept it anyway. I also found that accuracy improved up until the 7th pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = predictions.txt\n",
      "using no cache\n",
      "Reading datafile = stdin\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 175000\n",
      "power_t = 0.5\n",
      "Enabled reductions: gd, scorer-identity, binary, count_label\n",
      "Input label = simple\n",
      "Output pred = scalar\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       34\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000      536\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000      268\n",
      "0.000000 0.000000            8            8.0   1.0000   1.0000      202\n",
      "0.062500 0.125000           16           16.0   1.0000   1.0000      304\n",
      "0.062500 0.062500           32           32.0  -1.0000   1.0000      186\n",
      "0.140625 0.218750           64           64.0   1.0000   1.0000      108\n",
      "0.132812 0.125000          128          128.0  -1.0000  -1.0000       28\n",
      "0.128906 0.125000          256          256.0  -1.0000  -1.0000      382\n",
      "0.132812 0.136719          512          512.0  -1.0000  -1.0000      164\n",
      "0.139648 0.146484         1024         1024.0  -1.0000  -1.0000      294\n",
      "0.136719 0.133789         2048         2048.0   1.0000   1.0000       90\n",
      "0.134521 0.132324         4096         4096.0   1.0000   1.0000       98\n",
      "0.135132 0.135742         8192         8192.0   1.0000   1.0000      240\n",
      "0.134216 0.133301        16384        16384.0  -1.0000  -1.0000       84\n",
      "\n",
      "finished run\n",
      "number of examples = 25000\n",
      "weighted example sum = 25000.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.133160\n",
      "best constant = 0.000000\n",
      "best constant's loss = 1.000000\n",
      "total feature number = 6751885\n",
      "Accuracy: 0.8668053277868886\n"
     ]
    }
   ],
   "source": [
    "trainDataPath = \"train.vw\"\n",
    "testDataPath = \"test.vw\"\n",
    "nGramLength = 2\n",
    "nPasses = 7\n",
    "\n",
    "!rm sentiment.model\n",
    "!rm .cache\n",
    "!vw --random_seed 1 --ngram {nGramLength} --l2 0 --cache --final_regressor sentiment.model --loss_function logistic --passes {nPasses} < {trainDataPath} &> /dev/null\n",
    "!vw --testonly -i sentiment.model --predictions predictions.txt --binary  < {testDataPath}\n",
    "with open(\"predictions.txt\",\"r\") as predictionFile:\n",
    "    predictions = predictionFile.read().split(\"\\n\")\n",
    "with open(testDataPath,\"r\") as testFile:\n",
    "    labels = [line.split(\" | \")[0] for line in testFile.read().split(\"\\n\")]\n",
    "nCorrect = 0\n",
    "for i,prediction in enumerate(predictions[:-1]):\n",
    "    if prediction == labels[i]:\n",
    "        nCorrect += 1\n",
    "accuracy = nCorrect / len(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "- (BASELINE) Binary, n=1, data-ID=8: 0.8529258829646814\n",
    "- (BIGRAM) Binary, n=2 (CLI arg), data-ID=8: 0.8636054557817687\n",
    "- (BIGRAM) Binary, n=2 (NLTK), data-ID=9: 0.8425262989480421\n",
    "- (WORDCOUNT) Binary, n=1, data-ID=10: 0.8561257549698013\n",
    "- (NGRAM_COUNT) Binary, n=2 (NLTK), data-ID=11: 0.8452861885524579\n",
    "- (STOPWORDS REMOVED) Binary, n=1, data-ID=12: 0.8516859325626975\n",
    "- (STOPWORDS REMOVED BIGRAM) Binary, n=2 (CLI arg), data-ID=13: 0.8542458301667933\n",
    "- (BIGRAM x7) Binary, n=2 (CLI arg), data-ID=14, passes=7: 0.8636054557817687\n",
    "- (BIGRAM WITH LENGTH FEATURE x7) Binary, n=2 (CLI arg), data-ID=15, passes=7: 0.8668053277868886"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data-ID` attribute above refers to a version of the train/test sets in the `saved_sets` directory"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
